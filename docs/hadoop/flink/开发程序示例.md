# 开发程序示例

- 官方建议的开发工具是 IntelliJ IDEA
- 开发语言可以使用 Java 或者 Scala，~~一般建议使用 Scala，因为使用 Scala 实现函数式编程会比较简洁，Java 实现的代码逻辑就会比较笨重~~

## 引入依赖

从 1.11 版本开始，不管是 Java API 还是 Scala API，都需要引用一个包：

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-clients_2.11</artifactId>
    <version>1.12.2</version>
    <scope>provided</scope>
</dependency>
```

**Scala 版本**

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-scala_2.11</artifactId>
    <version>1.12.2</version>
    <scope>provided</scope>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-scala_2.11</artifactId>
    <version>1.12.2</version>
    <scope>provided</scope>
</dependency>
```

**Java 版本**

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-java</artifactId>
    <version>1.12.2</version>
    <scope>provided</scope>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-streaming-java_2.11</artifactId>
    <version>1.12.2</version>
    <scope>provided</scope>
</dependency>
```

## 开发步骤

开发 Flink 程序有固定的流程：

1. 获得一个执行环境；
2. 加载/创建初始化数据；
3. 指定操作数据的 Transaction 算子；
4. 指定计算好的数据的存放位置；
5. 调用 `execute()` 触发执行程序。

> 注意：Flink 程序是延迟计算的，只有最后调用 `execute()` 方法的时候才会真正触发执行程序。

## 流处理示例

需求分析：通过 Socket 手工实时产生一些单词，使用 Flink 实时接收数据，实现每隔 1s 对最近 2s 内的数据进行汇总计算，并且把时间窗口内计算的结果打印出来。

**Scala 版本**

```scala
package com.github.superzhc.flink.streaming

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.windowing.time.Time

object WordCount {
  def main(args: Array[String]): Unit = {
    val env:StreamExecutionEnvironment=StreamExecutionEnvironment.getExecutionEnvironment

    val text=env.socketTextStream("localhost",9090,"\n")

    import org.apache.flink.api.scala._
    val wordCount=text.flatMap(line=>line.split("\\s"))
      .map((_,1L))//
      .keyBy(0)//分组
      .timeWindow(Time.seconds(2),Time.seconds(1))//指定窗口大小，指定间隔时间
      .sum(1)//
//      .reduce((value1,value2)=>(value1._1,value1._2+value2._2))

    //打印到控制台
    wordCount.print().setParallelism(1)

    //执行任务
    env.execute("wordCount")
  }
}
```

**Java 版本**

```java
package com.github.superzhc.flink.streaming;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.ReduceFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.util.Collector;

/**
 * 2020年11月19日 superz add
 */
public class JWordCount
{
    public static void main(String[] args) throws Exception {
        // 获取运行环境
        StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(4);

        // 连接Socket获取输入的数据
        DataStreamSource<String> text=env.socketTextStream("localhost",9090,"\n");

        DataStream<Tuple2<String,Long>> wordCount=text.flatMap(new FlatMapFunction<String, Tuple2<String, Long>>()
        {
            @Override public void flatMap(String value, Collector<Tuple2<String, Long>> out) throws Exception {
                String[] words=value.split("\\s");
                for(String word:words){
                    out.collect(new Tuple2<>(word,1L));
                }
            }
        })
        .keyBy(0)//
        .window(SlidingProcessingTimeWindows.of(Time.seconds(3),Time.seconds(1)))//指定时间窗口大小为3s，指定时间间隔为1s
        .sum(1)//在这里使用sum或者reduce都可以
        /*.reduce(new ReduceFunction<Tuple2<String, Long>>()
        {
            @Override public Tuple2<String, Long> reduce(Tuple2<String, Long> value1, Tuple2<String, Long> value2)
                    throws Exception {
                return new Tuple2<>(value1.f0, value1.f1+value2.f1);
            }
        })*/
        ;

        // 把数据打印到控制台并设置并行度
        wordCount.print();

        // 执行代码
        env.execute("WordCount");

    }
}
```


## 批处理示例

**Scala 版本**

```scala
package com.github.superzhc.flink.batch

import org.apache.flink.api.scala.ExecutionEnvironment

object BatchWordCount {
  def main(args: Array[String]): Unit = {
    // 获取环境
    val env:ExecutionEnvironment=ExecutionEnvironment.getExecutionEnvironment

    // 获取文件内容
    val text=env.readTextFile("D://data/wordcount.txt")
    
    import org.apache.flink.api.scala._
    val counts=text.flatMap(_.split("\\W+"))//
      .filter(_.nonEmpty)//
      .map((_,1))//
      .groupBy(0)//
      .sum(1)
    
    counts.writeAsCsv("D://data/flink","\n"," ").setParallelism(1)
    env.execute("batch word count")
  }
}
```

**Java 版本**

```java
package com.github.superzhc.flink.batch;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.java.DataSet;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.operators.DataSource;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.util.Collector;

/**
 * 2020年11月19日 superz add
 */
public class JBatchWordCount
{
    public static void main(String[] args) throws Exception {
        // 获取运行环境
        ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();

        // 获取文件中的内容
        DataSource<String> text=env.readTextFile("D://data/wordcount.txt");
        DataSet<Tuple2<String,Integer>> counts=text.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>()
        {
            @Override public void flatMap(String value, Collector<Tuple2<String, Integer>> out) throws Exception {
                String[] tokens=value.split("\\W+");
                for(String token:tokens){
                    if(token.length()>0)
                        out.collect(new Tuple2<>(token,1));
                }
            }
        })//
        .groupBy(0)//
        .sum(1)//
        ;

        counts.writeAsCsv("D://data/flink","\n"," ").setParallelism(1);
        env.execute("batch word count");
    }
}
```