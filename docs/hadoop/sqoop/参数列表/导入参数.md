#### RDBMS参数

| 参数                           | 详细                                                         |
| ------------------------------ | ------------------------------------------------------------ |
| `--table <table-name>`         | 关系数据库表名，数据从该表中获取                             |
| `--where`                      | 指定导入时的 where 条件<br>例如 `--where "student_id<=500"`  |
| `--columns`                    | 指定要导入的字段名<br>例如 `--column class_id,class_name`    |
| `--query,-e <sql>`             | 从查询结果中导入数据，在查询语句中一定要有where条件且在where条件中需要包含 `\$CONDITIONS` <br/>示例：`--query "select * from t where \$CONDITIONS" --target-dir /tmp/t --hive-table t` <br/>`--query`后跟的查询语句可以是单引号引起来，也可以是双引号，无论何种引号都必须加上 `$CONDITIONS`;但当用双引号引用时，必须在 `$CONDITIONS` 前加上反斜杠即`\$CONDITIONS` |
| `--boundary-query <statement>` | 边界查询，也就是在导入前先通过 SQL 查询得到一个结果集，然后导入的数据就是该结果集内的数据，<br>格式如：`--boundary-query 'select id,no from t where id = 3'`，表示导入的数据为 `id=3` 的记录，<br>或者 `select min(<split-by>), max(<split-by>) from <table name>`，注意查询的字段中不能有数据类型为字符串的字段，否则会报错 |

#### HDFS 参数

| 参数                              | 说明                                                         |
| :-------------------------------- | :----------------------------------------------------------- |
| **输出格式化**                    |                                                              |
| `--enclosed-by <char>`            | 给字段值前后加上指定的字符，比如双引号，示例：`--enclosed-by '\"'` |
| `--escaped-by`                    | 给双引号做转义处理，如字段值为 `"测试"`，经过 `--escaped-by "\\"` 处理后，在 hdfs 中的显示值为： `\"测试\"`，对单引号无效 |
| `--fields-terminated-by`          | 默认情况下用 `,` 进行字段分隔                                |
| `--lines-terminated-by`           | 行分隔符                                                     |
| `--mysql-delimiters`              | Mysql默认的分隔符设置，字段之间以`,`隔开，行之间以换行`\n`隔开，默认转义符号是`\`，字段值以单引号`'`包含起来。 |
| `--optionally-enclosed-by <char>` | `--enclosed-by` 是强制给每个字段值前后都加上指定的符号，而`--optionally-enclosed-by`只是给带有双引号或单引号的字段值加上指定的符号，故叫可选的 |
| **存储格式**                      |                                                              |
| `--as-avrodatafile`               | 将数据导入到一个Avro数据文件中                               |
| `--as-parquetfile`                | 将数据导出成 parquet 文件                                    |
| `--as-sequencefile`               | 将数据导入到一个 sequence文件中                              |
| `--as-textfile`                   | 将数据导入到一个普通文本文件中，生成该文本文件后，可以在 Hive 中通过 SQL 语句查询出结果。 |
| **HDFS 目录设置**                 |                                                              |
| `--delete-target-dir`             | 删除目标目录                                                 |
| `--target-dir <dir>`              | 指定 HDFS 路径，默认数据会放在 `/user/{user.name}/{--table 参数指定表名}`目录下 |
| `--warehouse-dir <dir>`           | 与 `--target-dir` 不能同时使用，指定数据导入的存放目录，适用于HDFS 导入，不适合导入 Hive 目录 |
| **空值的处理**                    |                                                              |
| `--null-string <null-string>`     | 如果没有指定，则字符串 `null` 将被使用为导入数据的空值；用于处理数据库中文本类型的字段 |
| `--null-non-string <null-string>` | 如果没有指定，则字符串 `null` 将被使用为导入数据的空值；用于处理非文本类型的字段 |
| **其他**                          |                                                              |
| `--split-by <column>`             | 表的列名，用来切分工作单元，一般后面跟主键ID                 |
| `--direct`                        | 直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快 |
| `--direct-split-size`             | 在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。 |
| `--inline-lob-limit`              | 设定大对象数据类型的最大值                                   |
| `-z,--compress`                   | 压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件 |
| `--compression-codec`             | Hadoop 压缩编码，默认是gzip                                  |

#### Hive 参数

| 参数                         | 说明                                                         |
| :--------------------------- | :----------------------------------------------------------- |
| **`--hive-import`**          | 必须参数，指定导入 Hive，使用 Hive 的默认分隔符              |
| `--hive-home <dir>`          | Hive的安装目录，可以通过该参数覆盖掉默认的 Hive 目录         |
| `--hive-overwrite`           | 覆盖掉在 Hive 表中已经存在的数据                             |
| `--create-hive-table`        | 默认是 `false`，如果目标表已经存在了，那么创建任务会失败<br>当表在 Hive 不存在时，添加 `create-hive-table` 则在指定数据库中添加同名表 <br>不建议使用这个参数，因为导入的时候，会与 RDB 中的字段类型有所出入 |
| `--hive-database <db-name>`  | 默认是 `default`，Hive 的库名                                |
| `--hive-table <table-name>`  | 后面接要创建的 Hive 表名                                     |
| `--hive-partition-key <key>` | 指定分区表的字段                                             |
| `--hive-partition-value`     | 指定分区表的值                                               |
| `--hive-drop-import-delims`  | 将导入 Hive 中的字符串中包含的 `\n`，`\r`和`\01` 特殊字符丢弃 |
| `--hive-delims-replacement`  | 将导入 Hive 中的字符串中包含的 `\n`，`\r`和`\01` 特殊字符替换成用户自定义字符 |
| `--map-column-hive`          | 类型映射，例如：`--map-column-hive id=String,value=Int`      |

#### HBase 参数

| 参数                         | 说明                                                    |
| ---------------------------- | ------------------------------------------------------- |
| `--column-family <family>`   | 把内容导入到 HBase 中，默认是用主键作为 split 列        |
| `--hbase-create-table`       | 创建 Hbase 表                                           |
| `--hbase-row-key <col>`      | 指定字段作为 RowKey，如果输入表包含复合主键，用逗号隔开 |
| `--hbase-table <table-name>` | 指定 HBase 表                                           |

#### 增量导入参数

| 参数             | 说明                                                         |
| ---------------- | ------------------------------------------------------------ |
| `--check-column` | 用来指定一些列，这些列在增量导入时用来检查这些数据是否作为增量数据进行导入，和关系型数据库中的自增字段及时间戳类似。<br/>注意:这些被指定的列的类型不能使任意字符类型，如char、varchar等类型都是不可以的，同时`--check-column`可以去指定多个列 |
| `--incremental`  | 用来指定增量导入的模式，两种模式分别为Append和Lastmodified   |
| `--last-valu`    | 指定上一次导入中检查列指定字段最大值                         |
| `--append`       | 将数据追加到 HDFS 中已经存在的dataset中。使用该参数，Sqoop 将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。 |

#### 控制导入并行度参数

| 参数                                                | 详细                                                         |
| --------------------------------------------------- | ------------------------------------------------------------ |
| `-m,--num-mappers <mapper-nums>` | 控制导入时 MR 作业的 Map 任务数量，后接一个整数值，用来表示 MR 的并行度。默认是4个，最好不要将数字设置为高于集群的节点数。<br />在进行并行导入的时候，Sqoop 会使用 `split-by` 进行负载切分（按照表的 PK 进行切分），首先获取切分字段 Max 和 Min 值，再根据 Max 和 Min 值去进行切分，举例：student[1,1000]，Sqoop 会直接使用 4 个 Map 任务 `select * from student where id>=min and id<=max`(1,250),(250,500),(500,750),(750,1000)。如果切分字段不是均匀分布的话，会导致任务的不平衡。<br />注：Sqoop 目前不能使用多个字段作为切分字段 |

