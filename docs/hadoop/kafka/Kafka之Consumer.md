消费者（Consumer）负责订阅 Kafka 中的一个或多个主题（Topic），并从订阅的主题上拉取消息。消费者通过检查消息的偏移量来区分已经读取过的消息。**偏移量**是另一种元数据，它是一个不断递增的整数值，在创建消息时，Kafka 会把它添加到消息里。在给定的分区里，每个消息的偏移量都是唯一的。消费者把每个分区最后读取的消息偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或重启，它的读取状态不会丢失。

消费者是**消费者群组**的一部分，也就是说，会有一个或多个消费者共同读取一个主题。群组保证每个分区只能被一个消费者使用。消费者与分区之间的映射通常被称为消费者对分区的**所有权关系**。

通过这种方式，消费者可以消费包含大量消息的主题。而且，如果一个消费者失效，群组里的其他消费者可以接管失效消费者的工作。

## 概念

Kafka 消费者从属于**消费者群组**。一个群组里地消费者订阅地是同一个主题，每个消费者接收主题一部分分区的消息。

一个新的消费者加入群组时，它读取的是原本由其他消费者读取的消息；当一个消费者被关闭或发生崩溃时，它就离开群组，原本由它读取的分区将由群组里的其他消费者来读取。

在主题发生变化时，比如管理员添加新的分区，会发生分区重分配。

分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为**再均衡**，它为消费者群组带来了高可用性和伸缩性。在再均衡期间，消费者无法读取消息，造成整个群组一小段时间的不可用。另外，当分区被重新分配给另一个消费者时，消费者当前的读取状态会丢失，它由可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序。

消费者通过向被指派为**群组协调器**的broker（不同群组可以有不同的协调器）发送**心跳**来维持它们和群组的从属关系以及它们对分区的所有权关系。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区里的消息。消费者会在轮询消息（为了获取消息）或提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发一次再平衡。

如果一个消费者发生崩溃，并停止读取消息，群组协调器会等待几秒钟，确认它死亡了才会触发再均衡。在这几秒钟时间里，死亡的消费者不会读取分区里的消息。在清理消费者时，消费者会通知协调器它将要离开群组，协调器会立即触发一次再平衡，尽量降低处理停顿。

## 消费者客户端

Kafka 提供两种层次的客户端 API：如果消费者不太关心消息偏移量的处理，可以使用高级 API；如果想自定义消费逻辑，可以使用低级 API。

- **高级 API**：消费者客户端代码不需要管理偏移量的提交，并且采用了消费组的自动负载均衡的功能，确保消费者的增减不会影响消息的消费。高级 API 提供了从 Kafka 消费数据的高层抽象。
- **低级 API**：通常针对特殊的消费逻辑，比如消费者只想消费某些特定的分区。低级 API 的客户端代码需要自己实现一些和 Kafka 服务端相关的底层逻辑，比如选择分区的主副本、处理主副本的故障转移等。

### 创建 Kafka 消费者

在读取消息之前，需要先创建一个 KafkaConsumer 对象。

KafkaConsumer 有 3 个必要的属性：

- `bootstrap.servers`：指定了 Kafka 集群的连接字符串。
- `key.deserializer`：指定类把字节数组转换成 Java 对象
- `value.deserializer`：指定类把字节数组转换成 Java 对象
- `group.id`：不是必需的，不过一般情况认为时必需的。它指定了 KafkaConsumer 属于哪一个消费者群组。

示例：【创建一个 Kafka 对象】

```java
Properties props = new Properties();
props.put("bootstrap.servers", kafkaurl);
props.put("group.id", groupId);
props.put("key.deserializer", StringDeserializer.class.getName());
props.put("value.deserializer", StringDeserializer.class.getName());

this.consumer = new KafkaConsumer<String, String>(props);
```

### 订阅主题

创建好消费者之后，下一步可以开始订阅主题了。`subscribe()` 方法接受一个主题列表作为参数，使用如下：

```java
consumer.subscribe(Collections.singletonList("customerCountries"));
```

注：可以在调用 `subscribe()` 方法时传入一个正则表达式。正则表达式可以匹配多个主题，如果创建了新的主题，并且主题的名字与正则表达式匹配，那么会立即触发一次再均衡，消费者就可以读取新添加的主题。如果应用程序需要读取多个主题，并且可以处理不同类型的数据，那么这种订阅方式就很管用。在 Kafka 和其他系统之间复制数据时，使用正则表达式的方式订阅多个主题时很常见的做法。

示例：【要订阅所有与 test 相关的主题】

```java
// 使用正则表达式获取消息
consumer.subscribe(Pattern.compile("test.*"));
```

### 轮询

消息轮询是消费者 API 的核心，通过一个简单的轮询向服务器请求数据。一旦消费者订阅了主题，轮询就会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，开发者只需要使用一组简单的 API 来处理从分区返回的数据。

示例：【轮询获取数据】

```java
// 订阅之后，从 Kafka 中拉取数据
try{
    for (;;) {
        // 消费者必需持续对 Kafka 进行轮询，否则会被服务器认为已经死亡，它的分区会被移交给群组里的其他消费者。传给 poll() 方法的参数是一个超时时间，用于控制 poll() 方法的阻塞时间（再消费者的缓冲区里没有可用数据时会发生阻塞）。如果该参数被设为0，poll() 会立即返回，否则它会在指定的毫秒数内一直等待 broker 返回数据
        // poll() 方法返回一个记录列表。每条记录都包含了记录所属主题的信息、记录所在分区的信息、记录在分区里的偏移量，以及记录的键值对。
        ConsumerRecords<String, String> msglist = consumer.poll(1000);
        if (null != msglist && msglist.count() > 0) {
            // 遍历获取到的数据列表，逐条处理这些记录
            for (ConsumerRecord<String, String> record : msglist) {
                System.out.printf("转发消息信息：key=[%s],value=[%s]\n", record.key(), record.value());
            }
        }
    }
}catch(Exception e){
    // 关闭消费者。网络连接和socket也会随着关闭，并立即触发一次再平衡。而不是等待群组协调器发现它不再发送心跳并认定它已死亡，因为那样需要更长的时间，导致整个群组在一段时间内无法读取消息
    consumer.close();
}
```

由上可知，消费者实际上是一个长期运行的应用程序，它通过持续轮询向 Kafka 请求数据。

轮询不只是获取数据那么简单。在第一次调用新消费者的 `poll()` 方法时，它会负责查找 GroupCoordinator，然后加入群组，接受分配的分区。如果发生了再平衡，整个过程也是在轮询期间进行的。当然，心跳也是从轮询里发送出去的，所以要确保在轮询期间所做的任何处理工作都应该尽快完成。

### 消费者的配置

[Consumer配置](Kafka/Kafka配置/Consumer配置.md) 

Kafka 的文档列出了所有与消费者相关的配置说明，大部分参数都有合理的默认值，一般不需要修改它们，不过有些参数与消费者的性能和可用性有很大的关系，如下所示：

- `fetch.min.bytes`

  > 该属性指定了消费者从服务器获取记录的最小字节数。broker 在收到消费者的数据请求时，如果可用的数据量小于 `fetch.min.bytes` 指定的大小，那么它会等到有足够的可用数据时才把它返回给消费者。

- `fetch.max.wait.ms`

  > 用于指定 broker 的等待时间，默认是 500ms。如果没有足够的数据流入 Kafka，消费者获取最小数据量的要求就得不到满足，最终导致 500ms 的延迟。
  >
  > 示例：如果 `fetch.max.wait.ms`被设为 100ms，并且 `fetch.min.bytes` 被设为 1M，那么 Kafka 在收到消费者的请求后，要么返回 1MB 数据，要么在 100ms 后返回所有可用的数据，就看哪个条件先得到满足。

- `max.partition.fetch.bytes`

  > 该属性指定了服务器从每个分区里返回给消费者的最大字节数。它的默认值是 1MB，也就是说，`KafkaConsumer.poll()` 方法从每个分区里返回的记录最多不超过 `max.partition.fetch.bytes` 指定的字节。

- `session.timeout.ms`

- `auto.offset.reset`
  
  在 Kafka 中，每当消费者组内的消费者查找不到所记录的消费位移或发生位移越界时，就会根据消费者客户端参数 `auto.offset.reset` 的配置来决定从何处开始进行消费，这个参数的默认值为 `latest`。
  
  `auto.offset.reset` 的值可以为 earliest、latest 和 none 。关于 earliest 和 latest 的解释，官方描述的太简单，各含义在真实情况如下所示：
  
  - earliest ：当各分区下存在已提交的 offset 时，从提交的 offset 开始消费；无提交的 offset 时，从头开始消费。
  - latest ：当各分区下存在已提交的 offset 时，从提交的 offset 开始消费；无提交的 offset 时，消费该分区下新产生的数据。
  - none ：topic 各分区都存在已提交的 offset 时，从 offset 后开始消费；只要有一个分区不存在已提交的offset，则抛出异常。

- `enable.auto.commit`

  > 该属性指定了消费者是否自动提交偏移量，默认值是 true。为了尽量避免出现重复数据和数据丢失，可以把它设为 false，由自己控制何时提交偏移量。如果把它设为 true，还可以通过配置 `auto.commit.interval.ms` 属性来控制提交的频率

- `partition.assignment.strategy`

  > PartitionAssignor 根据给定的消费者和主题，决定哪些分区应该被分配给哪个消费者。
  >
  > Kafka 有两个默认的分配策略：
  >
  > - Range
  > - RoundRobin
  >
  > 通过设置 `partition.assignment.strategy` 来选择分区策略，默认使用的是 Range 策略。

- `client.id`

  > 该属性可以是任意字符串，broker 用它来标识从客户端发送过来的消息，通常被用在日志、度量指标和配额里。

- `max.poll.record`

  > 该属性用于控制单次调用 `call()` 方法能够返回的记录数量，可以控制在轮询里需要处理的数据量 

- `receive.buffer.bytes`和`send.buffer.bytes`

### 提交和偏移量

每次调用 `poll()` 方法，它总是返回由生产者写入 Kafka 但还没有被消费者读取过的记录，因此可以追踪到哪些记录是被群组里哪个消费者读取的。消费者可以使用 Kafka 来追踪消息在分区里的位置（偏移量）。

更新分区当前位置的操作叫做**提交**。

消费这通过往 `__consumer_offset` 的特殊主题发送消息，消息里包含每个分区的偏移量。如果消费者一直处于运行状态，那么偏移量就没有什么作用；不过，如果消费者发生崩溃或者由新的消费者加入群组，就会触发再均衡，完成再均衡之后，每个消费者可能分配到新的分区，而不是之前处理的那个，为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。

如果提交的偏移量小于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息就会被重复处理。

KafkaConsumer API 提供了很多种方式来提交偏移量

- 自动提交
- 提交当前偏移量
- 异步提交
- 同步和异步组合提交
- 提交特定的偏移量

