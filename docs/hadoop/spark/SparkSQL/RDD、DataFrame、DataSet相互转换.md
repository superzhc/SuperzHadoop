# RDD、DataFrame、DataSet 相互转换

## RDD

> 注意：并不是由任意类型对象组成的RDD都可以转化为DataFrame对象，只有当组成`RDD[T]`的每一个 `T` 对象内部具有公有且鲜明的字段结构时，才能隐式或显式地总结出创建 DataFrame 对象所必要的结构信息（Schema）进行转化。

Spark SQL支持将现有RDD转换为DataFrame的两种不同方法，其实也就是**隐式推断**或者**显式指定DataFrame对象的Schema**。

1. 使用反射机制（Reflection）推理出Schema（结构信息）
    采用这种方式转化为DataFrame对象，要求被转化的 `RDD[T]` 的类型T具有典型一维表的字段结构对象。
    Spark SQL的Scala接口支持自动将包含样例类对象的RDD转换为DataFrame对象。在样例类的声明中已预先定义了表的结构信息，内部通过反射机制即可读取样例类的参数的名称、类型，转化为DataFrame对象的Schema。
2. 由开发者指定Schema
    先构建一个Schema，然后将其应用到现有的 `RDD[Row]`。这种方式可以根据需求和数据结构构建Schema，而且需要将 `RDD[T]` 转化为Row对象组成的RDD(`RDD[Row]`)，这种方法的代码虽然多了点，但也提供了更高的自由度和灵活度。

自定义指定Schema的步骤：

1. 根据需求从源 RDD 转化为 `RDD[Row]`
2. 创建由符合在步骤1中创建的 RDD 中的 Rows 结构的 StructType 表示的模式
3. 通过 SparkSession 提供的 createDataFrame 方法将模式应用于行的 RDD

**Scala 版本**

```scala
val rdd=spark.sparkContext.textFile("D:\\test\\people.txt")

// 方法一：先转换成元组，再使用toDF添加Schema信息
val rdd1=rdd.map(ele=>(ele.split(",")(0),ele.split(",")(1).trim.toInt))
rdd1.toDF("name","age")//转df，指定字段名
rdd1.toDS()//转ds

// 方法二：定义样例类，再通过toDF直接转换样例类对象
case class Person(name:String,age:Int)
val rdd2=rdd.map(_.split(","))
.map(ele=>(Person(ele(0),ele(1).trim.toInt))).toDF()//将自定义类转df

// 方法三
val schemaString="name age"
val fields=schemaString.split(" ")
    .map(ele=>StructField(ele,StringType,nullable = true))
val schema=StructType(fields)
val rdd3=rdd.map(_.split(",")).map(ele=>Row(ele(0),ele(1).trim))
val df=spark.createDataFrame(rdd3,schema)//将StructType作用到rdd上，转df
```

**Java 版本**

*方式一*：

```java
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.Encoder;
import org.apache.spark.sql.Encoders;
 
// 通过一个文本文件创建Person对象的RDD
JavaRDD<Person> peopleRDD = spark.read()
  .textFile("examples/src/main/resources/people.txt")
  .javaRDD()
  .map(line -> {
    String[] parts = line.split(",");
	Person person = new Person();
	person.setName(parts[0]);
	person.setAge(Integer.parseInt(parts[1].trim()));
	return person;
  });
 
// 对JavaBeans的RDD指定schema得到DataFrame
Dataset<Row> peopleDF = spark.createDataFrame(peopleRDD, Person.class);
// 注册该DataFrame为临时视图
peopleDF.createOrReplaceTempView("people");
 
// 执行SQL语句
Dataset<Row> teenagersDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19");
// The columns of a row in the result can be accessed by field index
// 结果中的列可以通过属性的下标获取
Encoder<String> stringEncoder = Encoders.STRING();
Dataset<String> teenagerNamesByIndexDF = teenagersDF.map(
    row -> "Name: " + row.getString(0),
    stringEncoder);
teenagerNamesByIndexDF.show();
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+
// 或者通过属性的名字获取
Dataset<String> teenagerNamesByFieldDF = teenagersDF.map(
    row -> "Name: " + row.<String>getAs("name"),
    stringEncoder);
teenagerNamesByFieldDF.show();
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+
```

*方式二*：

```java

import java.util.ArrayList;
import java.util.List;
 
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
 
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
 
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
 
// 创建一个RDD
JavaRDD<String> peopleRDD = spark.sparkContext()
  .textFile("examples/src/main/resources/people.txt", 1)
  .toJavaRDD();
 
// 使用string定义schema
String schemaString = "name age";
 
// 基于用字符串定义的schema生成StructType
List<StructField> fields = new ArrayList<>();
for (String fieldName : schemaString.split(" ")) {
  StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true);
  fields.add(field);
}
StructType schema = DataTypes.createStructType(fields);

// 把RDD (people)转换为Rows
JavaRDD<Row> rowRDD = peopleRDD.map(record -> {
  String[] attributes = record.split(",");
  return RowFactory.create(attributes[0], attributes[1].trim());
});
 
// 对RDD应用schema
Dataset<Row> peopleDataFrame = spark.createDataFrame(rowRDD, schema);
 
// 使用DataFrame创建临时视图
peopleDataFrame.createOrReplaceTempView("people");
 
// 运行SQL查询
Dataset<Row> results = spark.sql("SELECT name FROM people");
 
// SQL查询的结果是DataFrames类型，支持所有一般的RDD操作
// 结果的列可以通过属性的下标或者名字获取
Dataset<String> namesDS = results.map(row -> "Name: " + row.getString(0), Encoders.STRING());
namesDS.show();
// +-------------+
// |        value|
// +-------------+
// |Name: Michael|
// |   Name: Andy|
// | Name: Justin|
// +-------------+
```

## DataFrame

```scala
val df=spark.sql("select date,status,api from data.api")

// 转换成RDD
df.rdd

//df转ds就太多了，先列举几个
df.as() map filter flatMap等等吧
```

## DataSet

**Scala 版本**

```scala
// 转换成RDD
ds.rdd
```

**Java 版本**

```java
Dataset<Row> ds=...
// 转换成 JavaRDD
ds.toJavaRDD()
```