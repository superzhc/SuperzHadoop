# Analysis 阶段分析

## 概述

在 SQL Parse 阶段，会使用 antlr4 将一条 SQL 语句解析成语法树，然后使用 antlr4 的访问者模式遍历生成语法树，也就是 Logical Plan。但其实 SQL Parse 这一阶段生成的 Logical Plan 是被称为 Unresolved Logical Plan。所谓 unresolved，就是说 SQL 语句中的对象都是未解释的。

比如说一条语句 `SELECT col FROM sales`，当用户不知道 col 的具体类型（Int，String，还是其他），甚至是否在 sales 表中是否有 col 这一个列的时候，就称之为是 Unresolved 的。

在 Analysis 阶段，主要就是解决这个问题，也就是将 Unresolved 的变成 Resolved 的。Spark SQL 通过使用 Catalyst Rule 和 Catalog 来跟踪数据源的 table 信息，并对 Unresolved 应用如下的 Rules（Rule可以理解为一条一条的规则，当匹配到树某些节点的时候就会被应用）：

- 从 Catalog 中，查询 Unresolved Logical Plan 中对应的关系（relations）
- 根据输入属性名（比如上述的 col 列），映射到具体的属性
- 确定哪些属性引用相同的值并赋予它们唯一的 ID
- 对数据进行强制转换，方便后续对表达式进行处理

而处理过后，就会真正生成一颗 Resolved Logical Plan。

## 详细解析

在 `org.apache.spark.sql.execution.QueryExecution` 会去调用 `org.apache.spark.sql.catalyst.Analyzer` 类来进行 Analysis，该类继承自 `org.apache.spark.sql.catalyst.rules.RuleExecutor`，这两个类自身实现了大量的 Rule，然后将这些 Rule 注册到 batches 变量中，部分代码如下：

```scala
class Analyzer(
    catalog: SessionCatalog,
    conf: SQLConf,
    maxIterations: Int)
  extends RuleExecutor[LogicalPlan] with CheckAnalysis {
  
    ......其他代码

    lazy val batches: Seq[Batch] = Seq(
    Batch("Hints", fixedPoint,
      new ResolveHints.ResolveBroadcastHints(conf),
      ResolveHints.ResolveCoalesceHints,
      ResolveHints.RemoveAllHints),
    Batch("Simple Sanity Check", Once,
      LookupFunctions),
    Batch("Substitution", fixedPoint,
      CTESubstitution,
      WindowsSubstitution,
      EliminateUnions,
      new SubstituteUnresolvedOrdinals(conf)),

    ......其他代码
}
```

上述的 batches 是由 Batch 的列表构成，而 Batch 的具体签名如下：

```scala
abstract class RuleExecutor[TreeType <: TreeNode[_]] extends Logging {
  ......其他代码
  protected case class Batch(name: String, strategy: Strategy, rules: Rule[TreeType]*)
  ......其他代码
}
```

一个 Batch 由策略 Strategy，和一组 Rule 构成，其中策略 Strategy 主要是区分迭代次数用的，rules 就是具体的应用规则了。

在 Analyzer 这个类中，存在着各种各样 Rule 的实现。然后最终，Analyzer 会去调用 `super.execute()` 方法，也就是调用父类（RuleExecutor）的方法执行具体逻辑。而父类又会去调用这个 batches 变量，循环来与 Sql Parse 阶段生成的 Unresolved Logical Plan 做匹配，匹配到了就执行具体的验证。

代码如下：

```scala
abstract class RuleExecutor[TreeType <: TreeNode[_]] extends Logging {
  def execute(plan: TreeType): TreeType = {
    var curPlan = plan
    val queryExecutionMetrics = RuleExecutor.queryExecutionMeter
    //遍历Analyzer中定义的batchs变量
    batches.foreach { batch =>
      val batchStartPlan = curPlan
      var iteration = 1
      var lastPlan = curPlan
      var continue = true

      // Run until fix point (or the max number of iterations as specified in the strategy.
	  //这里的continue决定是否再次循环，由batch的策略（固定次数或单次），以及该batch对plan的作用效果这两者控制
      while (continue) {
	    //调用foldLeft让batch中每条rule应用于plan，然后就是执行对应rule规则逻辑了
        curPlan = batch.rules.foldLeft(curPlan) {
          case (plan, rule) =>
            val startTime = System.nanoTime()
            val result = rule(plan)
            val runTime = System.nanoTime() - startTime

            if (!result.fastEquals(plan)) {
              queryExecutionMetrics.incNumEffectiveExecution(rule.ruleName)
              queryExecutionMetrics.incTimeEffectiveExecutionBy(rule.ruleName, runTime)
              logTrace(
                s"""
                  |=== Applying Rule ${rule.ruleName} ===
                  |${sideBySide(plan.treeString, result.treeString).mkString("\n")}
                """.stripMargin)
            }
            queryExecutionMetrics.incExecutionTimeBy(rule.ruleName, runTime)
            queryExecutionMetrics.incNumExecution(rule.ruleName)

            // Run the structural integrity checker against the plan after each rule.
            if (!isPlanIntegral(result)) {
              val message = s"After applying rule ${rule.ruleName} in batch ${batch.name}, " +
                "the structural integrity of the plan is broken."
              throw new TreeNodeException(result, message, null)
            }

            result
        }
        iteration += 1
		//策略的生效地方
        if (iteration > batch.strategy.maxIterations) {
          // Only log if this is a rule that is supposed to run more than once.
          if (iteration != 2) {
            val message = s"Max iterations (${iteration - 1}) reached for batch ${batch.name}"
            if (Utils.isTesting) {
              throw new TreeNodeException(curPlan, message, null)
            } else {
              logWarning(message)
            }
          }
          continue = false
        }

        if (curPlan.fastEquals(lastPlan)) {
          logTrace(
            s"Fixed point reached for batch ${batch.name} after ${iteration - 1} iterations.")
          continue = false
        }
        lastPlan = curPlan
      }

      if (!batchStartPlan.fastEquals(curPlan)) {
        logDebug(
          s"""
            |=== Result of Batch ${batch.name} ===
            |${sideBySide(batchStartPlan.treeString, curPlan.treeString).mkString("\n")}
          """.stripMargin)
      } else {
        logTrace(s"Batch ${batch.name} has no effect.")
      }
    }

    curPlan
  }

}
```

## Rule 简介

前面说到，在 Analyzer 中重写了 Batchs 变量，Batchs 包含多个 Batch，每个 Batch 又有多个 Rule。目前 Spark 中存在大量的 Rule，因此仅取一个通用的 Rule 进行分析，那就是 **ResolveRelations** 这个 Rule。代码如下：

```scala
object ResolveRelations extends Rule[LogicalPlan] {
    ......其他代码
    def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUp {
      case i @ InsertIntoTable(u: UnresolvedRelation, parts, child, _, _) if child.resolved =>
        EliminateSubqueryAliases(lookupTableFromCatalog(u)) match {
          case v: View =>
            u.failAnalysis(s"Inserting into a view is not allowed. View: ${v.desc.identifier}.")
          case other => i.copy(table = other)
        }
      case u: UnresolvedRelation => resolveRelation(u)
    }
	......其他代码
}
```

上述代码的逻辑就是匹配 UnresolvedRelation，然后递归去 Catalog 中获取对应的元数据信息，递归将它及子节点变成 Resolved。需要注意的是，SQL 中对应的有可能是文件数据，或是数据库中的表，亦或是视图；针对文件数据是不会立即转换成 Resolved，而表和视图则会立即转换。

> Catalog 是存储实体表元数据信息的，它的具体实现是 `org.apache.spark.sql.catalyst.catalog.SessionCatalog`