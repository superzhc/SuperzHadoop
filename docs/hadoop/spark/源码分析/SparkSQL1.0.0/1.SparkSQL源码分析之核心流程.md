# Spark SQL源码分析之核心流程

## 示例 Demo

```scala
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext._
case class Person(name: String, age: Int)
val people = sc.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(p => Person(p(0), p(1).trim.toInt))
people.registerAsTable("people")
val teenagers = sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")
teenagers.map(t => "Name: " + t(0)).collect().foreach(println)
```

## SQLContext

SQLContext是执行SQL的上下文对象，首先来看一下它Hold的有哪些成员：

### Catalog 

一个存储 `<tableName,logicalPlan>` 的 map 结构，查找关系的目录，注册表，注销表，查询表和逻辑计划关系的类。

![img](images/20140710180201297)

### SqlParser

Parse 传入的sql来对语法分词，构建语法树，返回一个logical plan

![img](images/20140710180238284)

### Analyzer 

  logical plan的语法分析器

![img](images/20140710180251700)

### Optimizer 

 logical Plan的优化器

![img](images/20140710180312636)

### LogicalPlan 

逻辑计划，由catalyst的TreeNode组成，可以看到有3种语法树

![img](images/20140710180048281)

### SparkPlanner 

包含不同策略的优化策略来优化物理执行计划

![img](images/20140710180346893)

### QueryExecution 

sql执行的环境上下文

![img](images/20140710180400528)

就是这些对象组成了Spark SQL的运行时，看起来很酷，有静态的metadata存储，有分析器、优化器、逻辑计划、物理计划、执行运行时。

## Spark SQL执行流程

![img](images/20140710180539822)

核心组件都是绿色的方框，每一步流程的结果都是蓝色的框框，调用的方法是橙色的框框。

先概括一下，大致的执行流程是：
Parse SQL -> Analyze Logical Plan -> Optimize Logical Plan -> Generate Physical Plan -> Prepareed Spark Plan -> Execute SQL -> Generate RDD

更具体的执行流程：

     sql or hql -> sql parser(parse)生成 unresolved logical plan -> analyzer(analysis)生成analyzed logical plan  -> optimizer(optimize)optimized logical plan -> spark planner(use strategies to plan)生成physical plan -> 采用不同Strategies生成spark plan -> spark plan(prepare) prepared spark plan -> call toRDD（execute（）函数调用） 执行sql生成RDD