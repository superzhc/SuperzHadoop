# 容错机制

在大多数情况下，Spark Streaming 数据都是通过网络接收的（除了使用 fileStream 数据源）。要让 Spark Streaming 的 RDD，都达到与普通 Spark 程序 RDD 相同的容错性，接收到的数据必须被复制到多个 Worker 节点上的 Executor 内存中，默认的复制因子是 2。

## 数据恢复

- 对于网络接收数据,在出现失败的事件时，有两种数据需要被恢复
  1. 数据已接收并复制
      一个 Worker 节点挂掉时，其他Worker节点上还有副本
  2. 数据已接收在缓存中**等待复制**
      需要重新从数据源获取一份数据
- 节点失败
  1. Worker 节点的失败
      任何一个运行了 Executor 的 Worker 节点挂掉，都会导致该节点上所有在内存中的数据都丢失。如果有 Receiver 运行在该 Worker 节点上的 Executor 中，那么缓存的和待复制的数据都会丢失。
  2. Driver 节点的失败
     如果运行 Spark Streaming 应用程序的 Driver 节点失败了，那么显然 SparkContext 会丢失，那么该 Application 的所有 Executor 的数据都会丢失。
     > 可以设置 driver 自动恢复

## 流式计算系统的容错语义

1. 最多一次：每条记录可能会被处理一次，或者根本就不会被处理。**可能有数据丢失**。
2. 至少一次：每条记录会被处理一次或多次，确保**零数据丢失**。但是可能会导致记录被**重复处理**几次。
3. 一次且仅一次：每条记录只会被处理一次。这是最强的一种容错语义。

## Spark Streaming 处理数据

1. 接收数据：使用 Receiver 或其他方式接收数据。
2. 计算数据：使用 DStream 的 transformation 操作对数据进行计算和处理。
3. 推送数据：最后计算出来的数据会被推送到外部系统，比如文件系统、数据库等

对应的语义保障

1. 接收数据：不同的数据源提供不同的语义保障
2. 计算数据：所有接收到的数据一定只会被计算一次，这是基于 RDD 的基础语义所保障的。
3. 推送数据：**output操作默认能确保至少一次的语义**，因为它依赖于 output 操作的类型，以及底层系统的语义支持（比如是否有事务支持等），但是用户可以实现它们自己的事务机制来确保一次且仅一次的语义

### 接收数据的容错语义

1. 基于容错的文件系统的数据源：一次且仅一次
2. 基于 Receiver 的数据源
   - 可靠的 Receiver
      这种 Receiver 会在接收到了数据，并且将数据复制之后，对数据源执行确认操作，数据源没有收到确认，会在 Receiver 重启之后重新发送数据
   - 不可靠的 Receiver
      这种 Receiver 不会发送确认操作，因此当 Worker 或者 Driver 节点失败的时候，可能会导致数据丢失。

| 部署场景             | Worker 失败                                                                                             | Driver 失败                                                                                                 |
| -------------------- | ------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| 没有开启预写日志机制 | 1. 不可靠 Receiver，会导致缓存数据丢失<br/>2. 可靠的 Receiver，可以保证数据零丢失<br/>3. 至少一次的语义 | 1. 不可靠 Receiver，缓存的数据全部丢失<br/>2. 任何 Receiver，过去接收的所有数据全部丢失<br/>3. 没有容错语义 |
| 开启了预写日志机制   | 1. 可靠 Receiver，零数据丢失<br/>2. 至少一次的语义                                                      | 1.可靠 Receiver 和文件，零数据丢失                                                                          |

> Kafka Direct API，可以保证所有从 Kafka 接收到的数据，都是一次且仅一次。
> 基于该语义保障，如果自己再实现一次且仅一次语义的 output 操作，那么就可以获得整个 Spark Streaming 应用程序的一次且仅一次的语义

### 输出数据的容错语义

output 操作，可以提供至少一次的语义：当 Worker 节点失败时，转换后的数据可能会被写入外部系统一次或多次

要真正获得一次且仅一次的语义

1. 幂等更新：多次写操作，都是写相同的数据，例如 saveAs 系列方法，总是写入相同的数据
2. 事务更新：所有的操作都应该做成事务的，从而让写入操作执行一次且仅一次。给每个 batch 的数据都赋予一个唯一的标识，然后更新的时候判定，如果数据库中还没有该唯一标识，那么就更新，如果有唯一标识，那么就不更新。
   ```scala
   dstream.foreachRDD { (rdd, time) =>
     rdd.foreachPartition { partitionIterator =>
       val partitionId = TaskContext.get.partitionId()
       val uniqueId = generateUniqueId(time.milliseconds, partitionId)
       // partitionId和foreachRDD传入的时间，可以构成一个唯一的标识
     }
   }
   ```